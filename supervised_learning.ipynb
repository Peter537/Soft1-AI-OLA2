{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Selection and Preparation\n",
    "\n",
    "This section will include:\n",
    "- loading the data,\n",
    "- exploring it,\n",
    "- preparing it for the modeling process which includes splitting the data into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13543</th>\n",
       "      <td>This could have been a rather entertaining fil...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16309</th>\n",
       "      <td>Take a look at those faces alongside the entra...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33898</th>\n",
       "      <td>Spoiler Alert I worked as an extra on this Lif...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38579</th>\n",
       "      <td>This film was released the year I was born and...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>First of all this movie is not a comedy; unles...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "13543  This could have been a rather entertaining fil...  negative\n",
       "16309  Take a look at those faces alongside the entra...  positive\n",
       "33898  Spoiler Alert I worked as an extra on this Lif...  positive\n",
       "38579  This film was released the year I was born and...  positive\n",
       "11031  First of all this movie is not a comedy; unles...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to columns `review` and `sentiment`. These sentiments can be either positive or negative. These values we have encoded as 1 for positive and 0 for negative, so we can use them in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      One of the other reviewers has mentioned that ...          1\n",
       "1      A wonderful little production. <br /><br />The...          1\n",
       "2      I thought this was a wonderful way to spend ti...          1\n",
       "3      Basically there's a family where a little boy ...          0\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...          1\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vertify the data afther mapping the sentiment:\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split into train, test and validation sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been slited into 3 sets: \n",
    "- training 60% (size 30000)\n",
    "- testing 20% (size 10000)\n",
    "- validation 20% (size 10000)\n",
    "\n",
    "The parameter `random_state` is a seed the makes the data shuffle at a random state. The `random_state` is set to 42 to have the same shuffle of the data each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text data, so its numerical and can be fed into the model\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all data sets X, which contains the reviews, we make these into vectors, so that our models can process them.\n",
    "\n",
    "`fit_transform` method is only used on the training set, so that the vectorizer learns the vocabulary of the training set. The testing and validation sets are only transformed, so that the vocabulary of the training set is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Training\n",
    "\n",
    "This section will include:\n",
    "- Three different models which will be trained and validated on the data\n",
    "- Explanations of the metrics used\n",
    "- Explanation of what hyperparameters are and how we adjust them\n",
    "- Explanation of what overfitting is and how we can spot it\n",
    "\n",
    "### Explanations of the metrics used:\n",
    "\n",
    "NB: _These metrics are used for both validation and testing of the models._\n",
    "\n",
    "- **Accuracy**: percentage of correct predictions. It gives an idear of how well the model is performing. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
    "<br> \n",
    "    `accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "- **Precision**: tells us the how many of the positive predictions were actually correct. It is calculated as the number of true positives divided by the number of true positives and false positives.\n",
    "<br>\n",
    "    `precision = TP / (TP + FP)`\n",
    "\n",
    "\n",
    "- **Recall**: tells us how many of the actual positive cases were predicted correctly. It is calculated as the number of true positives divided by the number of true positives and false negatives.\n",
    "<br>\n",
    "    `recall = TP / (TP + FN)`\n",
    "\n",
    "\n",
    "- **F1 Score**: A combinded mesurement of precision and recall - the harmonic mean of the two. It gives a more balanced view of the model's performance, beacuse it takes both false positives and false negatives into account.\n",
    "<br>\n",
    "    `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "\n",
    "All these metrics we are going to be using are from `sklearn.metrics` library.\n",
    "\n",
    "\n",
    "\n",
    "### Choice of models:\n",
    "- **Model 1: Multinomial Naive Bayes**\n",
    "    - Reasoning:\n",
    "\n",
    "- **Model 2: Logistic Regression**\n",
    "    - Reasoning:\n",
    "\n",
    "- **Model 3: Support Vector Machine**\n",
    "    - Reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the model\n",
    "def validate_model(model):\n",
    "    \n",
    "    y_val_pred = model.predict(X_val_vec)\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_precision = precision_score(y_val, y_val_pred)\n",
    "    val_recall = recall_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    return val_accuracy, val_precision, val_recall, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 traning & validation: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.8608\n",
      "Precision: 0.8856902710653498\n",
      "Recall: 0.8325103693462375\n",
      "F1: 0.8582773365913255\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec, y_train)\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = validate_model(mnb)\n",
    "print(f'Multinomial Naive Bayes:\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.8915\n",
      "Precision: 0.8909198113207547\n",
      "Recall: 0.8953189808413984\n",
      "F1: 0.8931139789183332\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000) # Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = validate_model(log_reg)\n",
    "print(f'Logistic Regression:\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC:\n",
      "Accuracy: 0.8921\n",
      "Precision: 0.8936758893280632\n",
      "Recall: 0.8931463559154651\n",
      "F1: 0.8934110441568704\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train_vec, y_train)\n",
    "\n",
    "val_accuracy, val_precision, val_recall, val_f1 = validate_model(svm)\n",
    "print(f'Linear SVC:\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Overfitting\n",
    "\n",
    "- **What is hyperparameters?**:\n",
    "<br>\n",
    "You can think of hyperparamters as a form of settings in ML models. These you have to define before the training begins. These can have a big impact on the performance of the model; however, its also important to be aware that it can have both negative and positive inpact. \n",
    "\n",
    "- **Which hyperparameters have been adjusted?**:\n",
    "    - Multinomial Naive Bayes:\n",
    "        - x\n",
    "        - y\n",
    "        - z\n",
    "\n",
    "    - Logistic Regression:\n",
    "        - x\n",
    "        - y\n",
    "        - z\n",
    "\n",
    "    - Support Vector Machine:\n",
    "        - x\n",
    "        - y\n",
    "        - z\n",
    "\n",
    "\n",
    "- **Overfitting**: \n",
    "<br>\n",
    "This is when e.g a model is fitted very well to the training data, but when it comes to new data, then the model can't predict the data do to it being to specific to the training data. \n",
    "<br><br> \n",
    "One way to discover if a model is overfitted is under the validation of the model when looking at the accuracy. If the accuracy has a low value, then the model is overfitting.\n",
    "<br>\n",
    "To ajust the model to not overfit, can depend on the model. For example in multinomial naive bayes, the Alpha hyperparameter can be adjusted to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "This section will include:\n",
    "- Final train & test/evaluation of the models\n",
    "- Selection of the best model.\n",
    "\n",
    "\n",
    "We have made a function (`evaluate_model`) to calculate these metrics (accuracy, precision, recall, F1 score) for each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    \n",
    "    y_test_pred = model.predict(X_test_vec)\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "    return test_accuracy, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that saves model results infomation like: accuracy, precision, recall and f1 score, to a text file:\n",
    "def save_resualts(name, **kwargs):\n",
    "    path = f'./models/{name}_results.txt'\n",
    "\n",
    "    with open(f'{path}', 'w') as f:\n",
    "        for key, value in kwargs.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB:\n",
      "Accuracy: 0.8632\n",
      "Precision: 0.8797848127457066\n",
      "Recall: 0.843818217900377\n",
      "F1: 0.8614262560777958\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(mnb)\n",
    "save_resualts('MultinomialNB', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'MultinomialNB:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(mnb, open('./models/MultinomialNB.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "Accuracy: 0.8911\n",
      "Precision: 0.8814213982232523\n",
      "Recall: 0.9057352649335185\n",
      "F1: 0.8934129392189488\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(log_reg)\n",
    "save_resualts('LogisticRegression', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'LogisticRegression:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(log_reg, open('./models/LogisticRegression.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC:\n",
      "Accuracy: 0.8892\n",
      "Precision: 0.8839617112717327\n",
      "Recall: 0.8979956340543759\n",
      "F1: 0.8909234101201023\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(svm)\n",
    "save_resualts('LinearSVC', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'LinearSVC:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(svm, open('./models/LinearSVC.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best fitting model is\n",
    "\n",
    "When looking at the metrics for the models, we can see based on the test set that the best model are **x** with the highest\n",
    " accuracy (x), precision (x), recall (x) and F1 score (x)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
