{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Selection and Preparation\n",
    "\n",
    "This section will include:\n",
    "- Loading the data.\n",
    "- Exploring the data.\n",
    "- Preparing the data for the modeling process, which includes splitting the data into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30419</th>\n",
       "      <td>I was delighted to see this gem of a film avai...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35397</th>\n",
       "      <td>I was blown away by the re-imagined Battlestar...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18209</th>\n",
       "      <td>The opening sequence alone is worth the cost o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19121</th>\n",
       "      <td>In a way, this film reminded me of \"Jumping Ja...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36351</th>\n",
       "      <td>Can you people please stop believing everythin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "30419  I was delighted to see this gem of a film avai...  positive\n",
       "35397  I was blown away by the re-imagined Battlestar...  positive\n",
       "18209  The opening sequence alone is worth the cost o...  positive\n",
       "19121  In a way, this film reminded me of \"Jumping Ja...  positive\n",
       "36351  Can you people please stop believing everythin...  negative"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to columns `review` and `sentiment`. These sentiments can be either positive or negative. These values we have encoded as 1 for positive and 0 for negative, so we can use them in the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>It is always difficult to bring a 450 pages bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24889</th>\n",
       "      <td>If I had realized John Wayne was in this movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39542</th>\n",
       "      <td>This really is a cringe making exercise. Dress...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17153</th>\n",
       "      <td>Well, I notice IMDB has not offered any plot i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48509</th>\n",
       "      <td>Anyone can make a movie these days. Budget, pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "873    It is always difficult to bring a 450 pages bo...          1\n",
       "24889  If I had realized John Wayne was in this movie...          0\n",
       "39542  This really is a cringe making exercise. Dress...          0\n",
       "17153  Well, I notice IMDB has not offered any plot i...          0\n",
       "48509  Anyone can make a movie these days. Budget, pr...          0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vertify the data afther mapping the sentiment:\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split into train, test and validation sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been slited into 3 sets: \n",
    "- training 60% (size 30000)\n",
    "- testing 20% (size 10000)\n",
    "- validation 20% (size 10000)\n",
    "\n",
    "The parameter `random_state` is a seed the makes the data shuffle at a random state. The `random_state` is set to 42 to have the same shuffle of the data each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text data, so its numerical and can be fed into the model\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the vectorized data:\n",
      "Training set: (30000, 82265)\n",
      "Validation set: (10000, 82265)\n",
      "Test set: (10000, 82265)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the vectorized data:\")\n",
    "print(\"Training set:\", X_train_vec.shape)\n",
    "print(\"Validation set:\", X_val_vec.shape)\n",
    "print(\"Test set:\", X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all data sets X, which contains the reviews, we make these into vectors, so that our models can process them. Important to note that this makes the data high-dimensional, which we can se on the shapes above.\n",
    "\n",
    "`fit_transform` method is only used on the training set, so that the vectorizer learns the vocabulary of the training set. The testing and validation sets are only transformed, so that the vocabulary of the training set is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Selection and Training\n",
    "\n",
    "This section will include:\n",
    "- Three different models which will be trained and validated on the data\n",
    "- Explanations of the metrics used\n",
    "- Explanation of what hyperparameters are and how we adjust them\n",
    "- Explanation of what overfitting is and how we can spot it\n",
    "\n",
    "### Explanations of the metrics used:\n",
    "\n",
    "NB: _These metrics are used for both validation and testing of the models._\n",
    "\n",
    "- **Accuracy**: percentage of correct predictions. It gives an idear of how well the model is performing. It is calculated as the number of correct predictions divided by the total number of predictions.\n",
    "<br> \n",
    "    `accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "- **Precision**: tells us the how many of the positive predictions were actually correct. It is calculated as the number of true positives divided by the number of true positives and false positives.\n",
    "<br>\n",
    "    `precision = TP / (TP + FP)`\n",
    "\n",
    "\n",
    "- **Recall**: tells us how many of the actual positive cases were predicted correctly. It is calculated as the number of true positives divided by the number of true positives and false negatives.\n",
    "<br>\n",
    "    `recall = TP / (TP + FN)`\n",
    "\n",
    "\n",
    "- **F1 Score**: A combinded mesurement of precision and recall - the harmonic mean of the two. It gives a more balanced view of the model's performance, beacuse it takes both false positives and false negatives into account. \n",
    "<br>\n",
    "    `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "\n",
    "\n",
    "Source: [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) & [F-Score: What are Accuracy, Precision, Recall, and F1 Score?](https://klu.ai/glossary/accuracy-precision-recall-f1)\n",
    "\n",
    "All these metrics we are going to be using are from `sklearn.metrics` library.\n",
    "\n",
    "\n",
    "\n",
    "### Choice of models:\n",
    "- **Model 1: Multinomial Naive Bayes (source: [Multinomial Naive Bayes - geeksforgeeks](https://www.geeksforgeeks.org/multinomial-naive-bayes/))**\n",
    "    - Reasoning: Should be suituable for text classification problems, like ours sentiment analysis problem, which is this process of finding if a message/text is positive or negative. This model dose this by making a base model, which is the probability of each word in the review, given the sentiment of the review. The model then uses this base model to predict the sentiment of a new review.\n",
    "\n",
    "- **Model 2: Logistic Regression (source: [Logistic Regression in Machine Learning - geeksforgeeks](https://www.geeksforgeeks.org/understanding-logistic-regression/))**\n",
    "    - Reasoning: The model should work well, when i comes to binary classification task or with other words, take the input data and determine which category it fit into (0 or 1). Furthermore, the model should be go at findeing out whihc features are important in the review, which should be good in prediction the sentiment of the review.\n",
    "\n",
    "- **Model 3: Support Vector Machine (source: [Support Vector Machine (SVM) Algorithm - geeksforgeeks](https://www.geeksforgeeks.org/support-vector-machine-algorithm/))**\n",
    "    - Reasoning: When using the LinearSVC model, we are using a linear kernel, which is a good choice for text classification and when the data is in a high-dimensional space. The data is in a high-dimensional space because we have vectorized the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results for GridSearchCV search for best hyperparameters\n",
    "def view_results(grid):\n",
    "    \n",
    "    results = grid.cv_results_\n",
    "    \n",
    "    print(\"GridSearchCV validation results:\")\n",
    "    print(\"\")\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Mean accuracy:\", results['mean_test_accuracy'])\n",
    "    print(\"Mean precision:\", results['mean_test_precision'])\n",
    "    print(\"Mean recall:\", results['mean_test_recall'])\n",
    "    print(\"Mean F1 score:\", results['mean_test_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring metrics used for GridSearchCV\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to predict and calculate the accuracy, precision, recall and F1 score\n",
    "def evaluate_model(model, X_v, y):\n",
    "    \n",
    "    y_pred = model.predict(X_v)\n",
    "\n",
    "    test_accuracy = accuracy_score(y, y_pred)\n",
    "    test_precision = precision_score(y, y_pred)\n",
    "    test_recall = recall_score(y, y_pred)\n",
    "    test_f1 = f1_score(y, y_pred)\n",
    "\n",
    "    return test_accuracy, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV validation results:\n",
      "\n",
      "Best parameters: {'alpha': 1.0}\n",
      "\n",
      "Mean accuracy: [0.8566     0.8602     0.86156667 0.86106667]\n",
      "Mean precision: [0.85855164 0.86813816 0.87485682 0.8817984 ]\n",
      "Mean recall: [0.85159035 0.84722735 0.84172298 0.83185612]\n",
      "Mean F1 score: [0.85504084 0.85753623 0.8579405  0.85605236]\n"
     ]
    }
   ],
   "source": [
    "# Determine best hyperparameters for Multinomial Naive Bayes\n",
    "mnb_param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "mnb_grid = GridSearchCV(MultinomialNB(), mnb_param_grid, cv=5, scoring=scoring, refit='f1')\n",
    "mnb_grid.fit(X_train_vec, y_train)\n",
    "\n",
    "view_results(mnb_grid)\n",
    "\n",
    "mnb = mnb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes (validation test set):\n",
      "Accuracy: 0.8608\n",
      "Precision: 0.8856902710653498\n",
      "Recall: 0.8325103693462375\n",
      "F1: 0.8582773365913255\n"
     ]
    }
   ],
   "source": [
    "# Validate the Multinomial Naive Bayes model on the validation set\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(mnb, X_val_vec, y_val)\n",
    "print(f'Multinomial Naive Bayes (validation test set):\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best Multinomial Naive Bayes model\n",
    "pickle.dump(mnb, open('./models/MultinomialNB.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV validation results:\n",
      "\n",
      "Best parameters: {'C': 10, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "\n",
      "Mean accuracy: [0.5457     0.6158     0.7894     0.78963333 0.87393333 0.87396667\n",
      " 0.8736     0.87376667 0.80613333 0.807      0.80566667 0.80573333\n",
      " 0.80583333 0.85773333 0.8577     0.85763333 0.85756667 0.85753333\n",
      " 0.88943333 0.8894     0.8894     0.8894     0.8894     0.89193333\n",
      " 0.89223333 0.89126667 0.89223333 0.89226667]\n",
      "Mean precision: [0.22755129 0.5735932  0.75253008 0.75215348 0.86200571 0.86196959\n",
      " 0.86906398 0.86920884 0.78793142 0.7908828  0.78772923 0.78779101\n",
      " 0.78793774 0.83629265 0.83658489 0.83638944 0.83628461 0.83627558\n",
      " 0.87772297 0.87751975 0.87747289 0.87751975 0.87747032 0.88458111\n",
      " 0.88529661 0.88399171 0.88529661 0.885355  ]\n",
      "Mean recall: [0.35187111 0.8823332  0.85816854 0.8597123  0.8884413  0.88857553\n",
      " 0.87776877 0.87797015 0.83427223 0.83125167 0.83333252 0.83339964\n",
      " 0.83339964 0.88729971 0.88676275 0.886897   0.886897   0.88682989\n",
      " 0.90327541 0.90347678 0.90354387 0.90347678 0.90354389 0.8998523\n",
      " 0.89958366 0.89904668 0.89958366 0.89958369]\n",
      "Mean F1 score: [0.27637162 0.69522158 0.80187857 0.80233954 0.87500233 0.8750485\n",
      " 0.87335954 0.8735334  0.81040438 0.81053564 0.8098591  0.80992418\n",
      " 0.8100028  0.86101366 0.86091387 0.86087527 0.86081959 0.86078266\n",
      " 0.89029142 0.89028378 0.89029118 0.89028378 0.89029132 0.89213257\n",
      " 0.89237184 0.89144908 0.89237184 0.89240129]\n"
     ]
    }
   ],
   "source": [
    "# Detiermine the best hyperparameters for the Logistic Regression model\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'saga'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'],\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring=scoring, refit='f1')\n",
    "logreg_grid.fit(X_train_vec, y_train)\n",
    "\n",
    "view_results(logreg_grid)\n",
    "\n",
    "logreg = logreg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (validation test set):\n",
      "Accuracy: 0.8943\n",
      "Precision: 0.8971054718477399\n",
      "Recall: 0.8937388899861742\n",
      "F1: 0.8954190165232018\n"
     ]
    }
   ],
   "source": [
    "# Validate the Logistic Regression model on the validation test set\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(logreg, X_val_vec, y_val)\n",
    "print(f'Logistic Regression (validation test set):\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best Logistic Regression model\n",
    "pickle.dump(logreg, open('./models/LogisticRegression.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV validation results:\n",
      "\n",
      "Best parameters: {'C': 0.1}\n",
      "\n",
      "Mean accuracy: [0.8548     0.89113333 0.89116667 0.87566667]\n",
      "Mean precision: [0.83070108 0.87744313 0.88470507 0.87187744]\n",
      "Mean recall: [0.88884349 0.90763844 0.89790563 0.87884304]\n",
      "Mean F1 score: [0.85876528 0.89226151 0.89124099 0.87531825]\n"
     ]
    }
   ],
   "source": [
    "# Detiermine the best hyperparameters for the SVM model\n",
    "svm_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(LinearSVC(), svm_param_grid, cv=5, scoring=scoring, refit='f1')\n",
    "svm_grid.fit(X_train_vec, y_train)\n",
    "\n",
    "view_results(svm_grid)\n",
    "\n",
    "svm = svm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC (validation test set):\n",
      "Accuracy: 0.8945\n",
      "Precision: 0.8917122752150117\n",
      "Recall: 0.9010468101915861\n",
      "F1: 0.8963552411828274\n"
     ]
    }
   ],
   "source": [
    "# Validate the SVM model on the validation test set\n",
    "val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(svm, X_val_vec, y_val)\n",
    "print(f'Linear SVC (validation test set):\\nAccuracy: {val_accuracy}\\nPrecision: {val_precision}\\nRecall: {val_recall}\\nF1: {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best SVM model\n",
    "pickle.dump(svm, open('./models/LinearSVC.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and Overfitting\n",
    "\n",
    "- **What is hyperparameters?**:\n",
    "<br>\n",
    "You can think of hyperparamters as a form of settings in ML models. These you have to define before the training begins. These can have a big impact on the performance of the model; however, its also important to be aware that it can have both negative and positive inpact. \n",
    "\n",
    "- **Which hyperparameters have been adjusted?**:\n",
    "\n",
    "    For all models we have used `GridSearchCV` (sources: [mygreatlearning](https://www.mygreatlearning.com/blog/gridsearchcv/) & [scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) to adjust the hyperparameters - which is a function from sklearn that searches for the best hyperparameters for the model. Here we get a result of different metrics such as accuracy, precision, recall and F1 score, where we can see how each hyperparameter affects the model. \n",
    "    \n",
    "    For each mode we have set `GridSearchCV's` parameters to:\n",
    "    \n",
    "    - first parameter is the model.\n",
    "\n",
    "    - `cv=5`: this is the number of folds in the cross-validation. We have set it to 5, because it is a good balance between the number of folds and the computational cost.\n",
    "\n",
    "    - `scoring=soring`: so we get accuracy, precision, recall and F1 score for each hyperparameter.\n",
    "\n",
    "    - `refit='f1'`: this is the metric we want to use to find the best hyperparameters. We have chosen F1 score, because it is a good balance between precision and recall.\n",
    "    \n",
    "    After fitting we use the `grid.best_params_` shows which hyperparameter values worked best on the training data. We also `grid.best_estimator_` to get the best model.\n",
    "\n",
    "    \n",
    "    Furthermore, we also use our own validation set to validate the model and look at the metrics to see if the model is overfitting. A combination of these two methods is used to adjust the hyperparameters.\n",
    "    <br>\n",
    "\n",
    "\n",
    "    **List of hyperparameters adjusted for each model**:\n",
    "    - **Multinomial Naive Bayes**:\n",
    "        - Alpha (1): this is a smoothing parameter, which we have adjusted to handle zero probabilities for words in reviews that happen to be unseen. \n",
    "\n",
    "    - **Logistic Regression**:\n",
    "        - C (10): This is a regularization parameter. We have adjusted this hyperparameter to prevent overfitting, because it can help with the misclassification of the training data in the margin. For example, if C is a high value, then the model trys to classify all the training data correctly, which can lead to overfitting. On the other hand, if C is low, then it allows more/some misclassification of the training data, which can prevent overfitting.\n",
    "\n",
    "        - Penalty (l2) & solver (saga): \n",
    "        Penalty has been adjusted for finding which (l1/l2) gives the best impact for the models proformance. We have adjusted solvers beacuse can give a better optimization of the model. Furthermore, saga also makes sence to be the preferred solver, because it should be good at handling sparse data, which is important for our text vectorized data.\n",
    "\n",
    "        - max_iter (1000): change to 1000, because of the data being high dimensional, which need more iterations to converge.\n",
    "\n",
    "    - **Support Vector Machine**:\n",
    "        - C (0.1): Same as in Logistic Regression, which we have adjusted to prevent overfitting.\n",
    "\n",
    "        - max_iter (1000): we dont adjust this hyperparameter, beacuse the default value is 1000.\n",
    "        \n",
    "- **Overfitting**: \n",
    "<br>\n",
    "Overfitting is for example when a model is fitted very well to the training data and only can understand the training data. If the model gets represented with new data, then the model can't predict the data correctly. \n",
    "<br><br> \n",
    "One way to discover if a model is overfitted is under the validation of the model when looking at the accuracy. If the accuracy has a low value, then the model is overfitting.\n",
    "<br>\n",
    "To prevent overfitting in models can depend on the type of model. For example in multinomial naive bayes, the Alpha hyperparameter can be adjusted to prevent overfitting. Antoher example is adjusting the C hyperparameter in Logistic Regression and Support Vector Machine, which can prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "This section will include:\n",
    "- Models evaluation on the test set.\n",
    "- Save the metric scores for each model.\n",
    "- Selection of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that saves model results infomation like: accuracy, precision, recall and f1 score, to a text file:\n",
    "def save_resualts(name, **kwargs):\n",
    "    path = f'./models/{name}_results.txt'\n",
    "\n",
    "    with open(f'{path}', 'w') as f:\n",
    "        for key, value in kwargs.items():\n",
    "            f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multinomial Naive Bayes evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB:\n",
      "Accuracy: 0.8632\n",
      "Precision: 0.8797848127457066\n",
      "Recall: 0.843818217900377\n",
      "F1: 0.8614262560777958\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(mnb, X_test_vec, y_test)\n",
    "save_resualts('MultinomialNB', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'MultinomialNB:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "Accuracy: 0.8891\n",
      "Precision: 0.8827425009738995\n",
      "Recall: 0.8993847985711451\n",
      "F1: 0.8909859431829352\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(logreg, X_test_vec, y_test)\n",
    "save_resualts('LogisticRegression', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'LogisticRegression:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Support Vector Machine evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC:\n",
      "Accuracy: 0.8911\n",
      "Precision: 0.8792242703533026\n",
      "Recall: 0.9087120460408811\n",
      "F1: 0.8937249926807846\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = evaluate_model(svm, X_test_vec, y_test)\n",
    "save_resualts('LinearSVC', accuracy=accuracy, precision=precision, recall=recall, f1=f1)\n",
    "\n",
    "print(f'LinearSVC:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best fitting model:\n",
    "\n",
    "When looking at the metrics for the models, we can see based on the test set that the best model are **Support Vector Machine** evaluation with the highest\n",
    " accuracy (0.891), precision (0.879), recall (0.908) and F1 score (0.893). However it is important to note that **Logistic Regression** also has a high accuracy (0.889), precision (0.882), recall (0.899) and F1 score (0.890)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
